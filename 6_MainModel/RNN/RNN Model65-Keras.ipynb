{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:#616161;color:white\">RNN Model</h1>\n",
    "\n",
    "Adapted from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#616161;color:white\">0. Setup</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Input Parameters</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Root path\n",
    "#root = \"C:/DS/Github/MusicRecommendation\"  # BA, Windows\n",
    "root = \"/home/badrul/git/EventPrediction\" # BA, Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Common Libraries</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import Tracer    # Used for debugging\n",
    "import logging\n",
    "from random import *\n",
    "\n",
    "# File and database management\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Date/Time\n",
    "import datetime\n",
    "import time\n",
    "#from datetime import timedelta # Deprecated\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt             # Quick\n",
    "%matplotlib inline\n",
    "\n",
    "# Misc\n",
    "import random\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.basicConfig(filename='RNN.log',level=logging.DEBUG)\n",
    "\n",
    "#-------------- Custom Libs -----------------#\n",
    "os.chdir(root)\n",
    "\n",
    "# Import the codebase module\n",
    "fPath = root + \"/1_codemodule\"\n",
    "if fPath not in sys.path: sys.path.append(fPath)\n",
    "\n",
    "# Custom Libs\n",
    "import coreCode as cc\n",
    "import lastfmCode as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Page Specific Libraries</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data science (comment out if not needed)\n",
    "#from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Load settings</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "settingsDict =  cc.loadSettings()\n",
    "dbPath = root + settingsDict['mainDbPath_xsml']\n",
    "fmSimilarDbPath = root + settingsDict['fmSimilarDbPath']\n",
    "fmTagsDbPath = root + settingsDict['fmTagsDbPath']\n",
    "trackMetaDbPath = root + settingsDict['trackmetadata']\n",
    "periodGranularity = int(settingsDict['periodGranularity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Set parameters</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model setup\n",
    "loadFromSave = False\n",
    "n_steps = 336 # timesteps\n",
    "n_hidden = 160 # hidden layer num of features\n",
    "n_classes = 2\n",
    "batch_size = 336*4\n",
    "learning_rate = 0.001\n",
    "cellType = \"BasicLSTMCell\"  # Choose: TimeFreqLSTMCell BasicLSTMCell\n",
    "\n",
    "#fieldList=\"UserID, t, HrsFrom5pm, isSun,isMon,isTue,isWed,isThu,isFri,isSat, t1,t2,t3,t4,t5,t10,t12hrs,t23_5hrs,t24hrs,t24_5hrs,t1wk,t2wks,t3wks,t4wks\"\n",
    "fieldList=\"UserID, t, HrsFrom5pm, isSun,isMon,isTue,isWed,isThu,isFri,isSat, t10,t12hrs,t24hrs,t1wk,t2wks,t3wks,t4wks\"\n",
    "\n",
    "# Training parameters\n",
    "training_iterations = 1\n",
    "sample_iteration = 1\n",
    "display_step = 5\n",
    "userSample =1\n",
    "timeStepSkip =5000\n",
    "\n",
    "tblName='tblTimeSeriesData'  # 'tblName='tblTimeSeriesDataDummy'\n",
    "trainModel = 1\n",
    "\n",
    "# Dummy test\n",
    "dummyTest = True\n",
    "if dummyTest: \n",
    "    tblName='tblTimeSeriesDataDummy'  # 'tblName='tblTimeSeriesDataDummy'\n",
    "    fieldList=\"UserID, t, HrsFrom5pm, isSun,isMon,isTue,isWed,isThu,isFri,isSat,t1wk,t2wks,t3wks,t4wks\"\n",
    "    trainModel = 1  # Which RNN model to use\n",
    "    n_steps = 5 # has to be 1 for train model 2\n",
    "    batch_size = 5  # Num of 'time steps' in model 2\n",
    "    training_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#616161;color:white\">1. Build Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases,n_steps):\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    if trainModel == 1:\n",
    "        x = tf.unstack(x, n_steps, 1)  # See https://stackoverflow.com/questions/45278276/tensorflow-lstm-dropout-implementation-shape-problems/45279243#45279243\n",
    "    elif trainModel == 2:\n",
    "        x = tf.unstack(x, batch_size, 0)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    #lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    if cellType == \"BasicLSTMCell\":\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    elif cellType == \"TimeFreqLSTMCell\":\n",
    "        lstm_cell =rnn.TimeFreqLSTMCell(n_hidden, use_peepholes=True, feature_size= 22, forget_bias=1.0)\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    elif cellType == \"GridLSTMCell\":\n",
    "        lstm_cell =rnn.GridLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)        \n",
    "    else:\n",
    "        print(\"Did not recognize {}\".format(cellType))\n",
    "    # Get lstm cell output\n",
    "    \n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "def buildGraph(n_steps,n_input):\n",
    "    global x, y, pred, cost, optimizer,accuracy\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # tf Graph input\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = RNN(x, weights, biases,n_steps)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_input = len(fieldList.split(\",\"))-2 # -2 as we drop UserID and t\n",
    "\n",
    "# Build graph\n",
    "buildGraph(n_steps,n_input = n_input)\n",
    "# Initializing the variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "if loadFromSave:\n",
    "    saver.restore(sess,'./3_Data/saves/model.ckpt')\n",
    "else:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#616161;color:white\">2. Train Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "def trainModel2(_X, _Y, sess,training_iterations = 5):\n",
    "    # Training cycle\n",
    "    l=np.shape(_X)[0]\n",
    "    predictions=np.zeros([l,n_classes])\n",
    "    idx=0\n",
    "    \n",
    "    for i in range(training_iterations):\n",
    "        if (training_iterations % 10) == 0: print(\"Now on iteration {}\".format(i))\n",
    "        #logging.info(\"Now on iteration {}\".format(i))\n",
    "        # Loop over all rows in order of earliest to latest\n",
    "        for pos in range(0+batch_size, l,3):\n",
    "            if (pos % 1000) == 0: \n",
    "                print(\"Now on pos {} of {} ({}%)\".format(pos,l,round((pos/l)*100,2)))\n",
    "                logging.info(\"Now on pos {} of {} ({}%)\".format(pos,l,round((pos/l)*100,2)))\n",
    "            \n",
    "            # For each row, collect the previous batch_size num of rows\n",
    "            batch_x = _X[pos-batch_size:pos].reshape((batch_size, n_steps, n_input)) \n",
    "            batch_y = _Y[pos].reshape((-1, n_classes)) \n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Test at the same time\n",
    "            p = sess.run(pred, feed_dict={x: batch_x, y: batch_y})\n",
    "            predictions[idx]=p=p.reshape(-1,n_classes)[-1]\n",
    "            \n",
    "    \n",
    "        # Calculate loss & accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "        loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        predictions = np.argmax(predictions,1).reshape(-1,1)\n",
    "        labels=np.argmax(_Y,1).reshape(-1,1)\n",
    "        \n",
    "        print (\"Iter {}. Minibatch Loss={:.6f}\".format(i, loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        Tracer()()\n",
    "        print(metrics.classification_report(labels,predictions))  # Need to feed it yTest not yTest_OneHot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "def trainModel1(_X, _Y, sess,_iterations = 5, numOfPeriods = 10, trainPeriods = None):\n",
    "    # Num of periodss = batch size\n",
    "    \n",
    "    # Training cycle\n",
    "    totalRows=np.shape(_X)[0]\n",
    "    XCols=np.shape(_X)[1]\n",
    "    YCols=np.shape(_Y)[1]\n",
    "    depth = n_steps\n",
    "    \n",
    "    # If no trainPeriods were provided generate your own\n",
    "    if trainPeriods is None:\n",
    "        # Select periods where we will always get enough history to go with it\n",
    "        trainPeriods = random.sample(range(batch_size+depth, totalRows), numOfPeriods)\n",
    "    else:\n",
    "        trainPeriods = trainPeriods + batch_size+depth-1\n",
    "        numOfPeriods = len(trainPeriods)\n",
    "    \n",
    "    \n",
    "    for i in range(_iterations):\n",
    "        if (_iterations % 1) == 0: print(\"Now on iteration {}\".format(i))\n",
    "        #logging.info(\"Now on iteration {}\".format(i))\n",
    "        \n",
    "        # Pre-Initialize batch arrays\n",
    "        batch_x=np.zeros([numOfPeriods,depth,XCols])\n",
    "        batch_y=np.zeros([numOfPeriods,YCols])\n",
    "        \n",
    "        batch_row =0\n",
    "        for idx1 in trainPeriods:            \n",
    "            # Each period will have one batch\n",
    "            # Logging\n",
    "            if (idx1 % 1) == 0: \n",
    "                timeNow =str(datetime.datetime.now())\n",
    "                #print(\"{} Now training on Period {} ({}%)\".format(timeNow,idx1,round((batch_row/numOfPeriods)*100,2)))\n",
    "                logging.info(\"{} Now training on Period {} ({}%)\".format(timeNow, idx1,round((batch_row/numOfPeriods)*100,2)))\n",
    "\n",
    "            batch_x[batch_row] = _X[idx1-depth:idx1].reshape(1,depth,XCols)\n",
    "            batch_y[batch_row] = _Y[idx1]\n",
    "            batch_row +=1\n",
    "            \n",
    "         # Train\n",
    "        batch_y = batch_y.reshape((-1, YCols))\n",
    "        \n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        # Calculate loss & accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "        loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "        print (\"Iter {}. Minibatch Loss={:.6f}\".format(i, loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        \n",
    "        predictions = 1*sess.run(pred, feed_dict={x: batch_x, y: batch_y})\n",
    "        predictions = np.argmax(predictions,1).reshape(-1,1)\n",
    "        batch_y = np.argmax(batch_y,1).reshape(-1,1)\n",
    "        print(metrics.classification_report(batch_y,predictions))  # Need to feed it yTest not yTest_OneHot here\n",
    "        TestHiddenPeriods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-08 00:50:49.210706 Now processing sample 0\n",
      "2017-08-08 00:50:49.213495 Now processing User 3\n",
      "Now on iteration 0\n",
      "Iter 0. Minibatch Loss=1.768659, Training Accuracy= 0.30000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.40      0.36         5\n",
      "          1       0.25      0.20      0.22         5\n",
      "\n",
      "avg / total       0.29      0.30      0.29        10\n",
      "\n",
      "Now on iteration 1\n",
      "Iter 1. Minibatch Loss=1.577910, Training Accuracy= 0.30000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.40      0.36         5\n",
      "          1       0.25      0.20      0.22         5\n",
      "\n",
      "avg / total       0.29      0.30      0.29        10\n",
      "\n",
      "Now on iteration 2\n",
      "Iter 2. Minibatch Loss=1.491086, Training Accuracy= 0.50000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.40      0.44         5\n",
      "          1       0.50      0.60      0.55         5\n",
      "\n",
      "avg / total       0.50      0.50      0.49        10\n",
      "\n",
      "Now on iteration 3\n",
      "Iter 3. Minibatch Loss=1.471371, Training Accuracy= 0.40000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.20      0.25         5\n",
      "          1       0.43      0.60      0.50         5\n",
      "\n",
      "avg / total       0.38      0.40      0.38        10\n",
      "\n",
      "Now on iteration 4\n",
      "Iter 4. Minibatch Loss=1.452572, Training Accuracy= 0.30000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         5\n",
      "          1       0.38      0.60      0.46         5\n",
      "\n",
      "avg / total       0.19      0.30      0.23        10\n",
      "\n",
      "Now on iteration 5\n",
      "Iter 5. Minibatch Loss=1.391321, Training Accuracy= 0.30000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         5\n",
      "          1       0.38      0.60      0.46         5\n",
      "\n",
      "avg / total       0.19      0.30      0.23        10\n",
      "\n",
      "Now on iteration 6\n",
      "Iter 6. Minibatch Loss=1.271163, Training Accuracy= 0.30000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         5\n",
      "          1       0.38      0.60      0.46         5\n",
      "\n",
      "avg / total       0.19      0.30      0.23        10\n",
      "\n",
      "Now on iteration 7\n",
      "Iter 7. Minibatch Loss=1.100281, Training Accuracy= 0.30000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         5\n",
      "          1       0.38      0.60      0.46         5\n",
      "\n",
      "avg / total       0.19      0.30      0.23        10\n",
      "\n",
      "Now on iteration 8\n",
      "Iter 8. Minibatch Loss=0.911369, Training Accuracy= 0.50000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.20      0.29         5\n",
      "          1       0.50      0.80      0.62         5\n",
      "\n",
      "avg / total       0.50      0.50      0.45        10\n",
      "\n",
      "Now on iteration 9\n",
      "Iter 9. Minibatch Loss=0.758570, Training Accuracy= 0.50000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.20      0.29         5\n",
      "          1       0.50      0.80      0.62         5\n",
      "\n",
      "avg / total       0.50      0.50      0.45        10\n",
      "\n",
      "100 Hidden Periods\n",
      "\n",
      "Cell type= BasicLSTMCell, learning_rate = 0.001, Iterations = 10, batch size = 5, Steps = 5, Hidden Layers = 160, Classes = 2\n",
      "\n",
      "(100, 1) (100, 1)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.46      0.56        69\n",
      "          1       0.33      0.58      0.42        31\n",
      "\n",
      "avg / total       0.59      0.50      0.52       100\n",
      "\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "## Begin training\n",
    "numOfPeriods =20\n",
    "\n",
    "for s in range(sample_iteration):\n",
    "    timeNow =str(datetime.datetime.now())\n",
    "    print('{} Now processing sample {}'.format(timeNow,s))\n",
    "    logging.info('{} Now processing sample {}'.format(timeNow, s))\n",
    "    \n",
    "    if dummyTest:\n",
    "        users = pd.DataFrame(data={'userID': [3]})\n",
    "    else:\n",
    "        users=cc.getUsers(dbPath).sample(userSample)\n",
    "        \n",
    "    for usr in users.itertuples():\n",
    "        timeNow =str(datetime.datetime.now())\n",
    "        print('{} Now processing User {}'.format(timeNow, usr.userID))\n",
    "        logging.info('{} Now processing User {}'.format(timeNow, usr.userID))\n",
    "        xTrain, yTrain_onehot, xTest, yTest_onehot = cc.getHiddenPeriodsData(dbPath,tblName,fieldList,oneHot=True,periodGranularity=periodGranularity,userIDs=[usr.userID])\n",
    "        \n",
    "        if xTrain is not None:\n",
    "            if np.shape(yTrain_onehot)[1] !=1:  # Results have to have both 0's and 1's in them\n",
    "                if trainModel ==1:\n",
    "                    #trainModel1(xTrain, yTrain_onehot, sess,training_iterations, numOfPeriods=numOfPeriods)\n",
    "                elif trainModel ==2:\n",
    "                    trainModel2(xTrain, yTrain_onehot, sess,training_iterations)\n",
    "        saver.save(sess,\"./3_Data/saves/model.ckpt\")\n",
    "\n",
    "TestHiddenPeriods()       \n",
    "print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:blue;color:white\"></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#616161;color:white\">3. Test Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def TestPredictions2(_X,_Y,sess):\n",
    "    \n",
    "    l=np.shape(_X)[0]\n",
    "    labels=np.zeros([l,n_classes]) # Calculate the number of lead Y's we will end up with\n",
    "    predictions=np.zeros([l,n_classes])\n",
    "    \n",
    "    # Testing cycle\n",
    "    print(\"Now testing {} rows\".format(l))\n",
    "    logging.info(\"Now testing {} rows\".format(l))\n",
    "    \n",
    "    # Pad rows at the beginning so we can get a prediction for every entry\n",
    "    padX=np.zeros([batch_size-1,_X.shape[1]])\n",
    "    _Y = _Y.reshape(-1,n_classes)\n",
    "    padY=np.zeros([batch_size-1,n_classes])\n",
    "    \n",
    "    _X = np.append(padX, _X, axis=0)\n",
    "    _Y = np.append(padY, _Y, axis=0)\n",
    "    l=np.shape(_X)[0]  # Update length\n",
    "    \n",
    "    # Pre allocate arrays and regain sanity\n",
    "    idx =0\n",
    "    for pos in range(0+batch_size, l):\n",
    "        \n",
    "        if (pos % 10000) == 0: \n",
    "            print(\"Now on pos {} of {} ({}%)\".format(pos,l,round((pos/l)*100,2)))\n",
    "            #logging.info(\"Now on pos {} of {} ({}%)\".format(pos,l,round((pos/l)*100,2)))\n",
    "\n",
    "        # For each row, collect the previous batch_size num of rows\n",
    "        batch_x = _X[pos-batch_size:pos]\n",
    "        batch_y = _Y[pos]\n",
    "        #if np.mod(len(batch_x),batch_size) == 0:batch_x, batch_y, _ = cc.padRows(batch_x, batch_y, batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))  # Rehsape into 3d, even though n_steps is 1            \n",
    "        batch_y = batch_y.reshape((-1, n_classes))  # Rehsape into 3d, even though n_steps is 1            \n",
    "\n",
    "        # Predict!\n",
    "        p = 1*sess.run(pred, feed_dict={x: batch_x})\n",
    "        p=p.reshape(-1,n_classes)\n",
    "        \n",
    "        #print(_Y[pos], batch_y)\n",
    "        predictions[idx] = p[-1]\n",
    "        labels[idx] = batch_y[-1]\n",
    "        idx+=1\n",
    "    \n",
    "    \n",
    "    # Remove padding and return predictions\n",
    "    predictions = np.argmax(predictions,1)\n",
    "    predictions = predictions.reshape(-1,1)\n",
    "    labels = np.argmax(labels,1)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    \n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "def TestPredictions1(_X, _Y, sess, numOfPeriods = 10, testPeriods = None):\n",
    "    # Training cycle\n",
    "    totalRows=np.shape(_X)[0]\n",
    "    XCols=np.shape(_X)[1]\n",
    "    YCols=np.shape(_Y)[1]\n",
    "    depth = n_steps\n",
    "    \n",
    "    # If no test periods were provided generate your own\n",
    "    if testPeriods is None:\n",
    "        # Select periods where we will always get enough history to go with it\n",
    "        testPeriods = random.sample(range(batch_size+depth, totalRows), numOfPeriods)\n",
    "    else:\n",
    "        testPeriods = testPeriods + batch_size+depth-1\n",
    "        numOfPeriods = len(testPeriods)\n",
    "     \n",
    "    # Pre-Initialize batch arrays\n",
    "    batch_x=np.zeros([numOfPeriods,depth,XCols])\n",
    "    batch_y=np.zeros([numOfPeriods,YCols])\n",
    "\n",
    "    batch_row =0\n",
    "    for idx1 in testPeriods:            \n",
    "        if (idx1 % 1) == 0: \n",
    "            timeNow =str(datetime.datetime.now())\n",
    "            #print(\"{} Now testing on period {} ({}%)\".format(timeNow,idx1,round((batch_row/numOfPeriods)*100,2)))\n",
    "            logging.info(\"{} Now testing period {} ({}%)\".format(timeNow, idx1,round((batch_row/numOfPeriods)*100,2)))\n",
    "\n",
    "        batch_x[batch_row] = _X[idx1-depth:idx1].reshape(1,depth,XCols)\n",
    "        batch_y[batch_row] = _Y[idx1]\n",
    "        batch_row +=1\n",
    "\n",
    "    # Predict for this period\n",
    "    predictions = 1*sess.run(pred, feed_dict={x: batch_x, y: batch_y})\n",
    "    predictions = np.argmax(predictions,1).reshape(-1,1)\n",
    "    batch_y = np.argmax(batch_y,1).reshape(-1,1)\n",
    "    return predictions, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Test hidden periods</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Hidden Periods\n",
      "\n",
      "Cell type= BasicLSTMCell, learning_rate = 0.001, Iterations = 10, batch size = 5, Steps = 5, Hidden Layers = 160, Classes = 2\n",
      "\n",
      "(100, 1) (100, 1)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.88      0.73        64\n",
      "          1       0.27      0.08      0.13        36\n",
      "\n",
      "avg / total       0.50      0.59      0.51       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def TestHiddenPeriods(hiddenTestPeriods=50):\n",
    "\n",
    "    print('{} Hidden Periods\\n'.format(hiddenTestPeriods))\n",
    "    print (\"Cell type= {}, learning_rate = {}, Iterations = {}, batch size = {}, Steps = {}, Hidden Layers = {}, Classes = {}\\n\".format(cellType,learning_rate,training_iterations,batch_size, n_steps ,n_hidden,n_classes))\n",
    "\n",
    "    if trainModel == 1:\n",
    "        predictions,labels = TestPredictions1(xTrain,yTrain_onehot,sess,numOfPeriods=hiddenTestPeriods)\n",
    "    elif trainModel == 2:\n",
    "        predictions,labels = TestPredictions2(xTrain,yTrain_onehot,sess)\n",
    "\n",
    "    predictions = predictions.reshape(-1,1)\n",
    "    labels = labels.reshape(-1,1)\n",
    "\n",
    "    print(np.shape(labels),np.shape(predictions))    \n",
    "    print(metrics.classification_report(labels,predictions))  # Need to feed it yTest not yTest_OneHot here\n",
    "\n",
    "TestHiddenPeriods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#white; color:#008000; font-family: 'Courier New, Monospace;font-weight: bold\">Test hidden users</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get hidden users\n",
    "users=cc.getUsers(dbPath).sample(2)\n",
    "u=users.userID.values\n",
    "_,_,xTest, yTest_onehot = cc.getHiddenPeriodsData(dbPath,tblName,fieldList,oneHot=True,periodGranularity=periodGranularity,userIDs=u)\n",
    "print ('{} users selected for testing. Total rows {}'.format(len(u), len(xTest)))\n",
    "\n",
    "xTest2, yTest2_onehot, testDf2 = cc.getHiddenUsersData(dbPath,tblName,fieldList,oneHot= True,firstNPerc=0.5,periodGranularity=periodGranularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nHidden Users')\n",
    "predictions = getTestPredictions(xTest2,yTest2_onehot)\n",
    "print(metrics.classification_report(yTest2_onehot[:,1],predictions))  # Need to feed it yTest not yTest_OneHot here\n",
    "print(np.shape(xTest2),np.shape(yTest2_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#616161;color:white\">Appendices</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(seq_len, normalise_window):\n",
    "    f = open(\"3_Data/sinwave.csv\", 'rb').read()\n",
    "    data = f.decode().split('\\n')\n",
    "\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "\n",
    "    result = np.array(result)\n",
    "\n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    np.random.shuffle(train)\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "seq_len = 1\n",
    "xdTrain, ydTrain, Xd_test, yd_test = load_data(seq_len, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buildGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c2cc9ec43994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Build graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbuildGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initializing the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'buildGraph' is not defined"
     ]
    }
   ],
   "source": [
    "# Dummy test\n",
    "n_steps = 1 # timesteps\n",
    "n_hidden = 160 # hidden layer num of features\n",
    "n_classes = 1\n",
    "batch_size = 20 #1344\n",
    "training_iterations=100\n",
    "learning_rate = 0.001\n",
    "cellType = \"BasicLSTMCell\"  # Choose: TimeFreqLSTMCell BasicLSTMCell\n",
    "\n",
    "#fieldList=\"UserID, t, HrsFrom5pm, isSun,isMon,isTue,isWed,isThu,isFri,isSat, t1,t2,t3,t4,t5,t10,t12hrs,t23_5hrs,t24hrs,t24_5hrs,t1wk,t2wks,t3wks,t4wks\"\n",
    "fieldList=\"UserID, t, HrsFrom5pm, isSun,isMon,isTue,isWed,isThu,isFri,isSat, t10,t12hrs,t24hrs,t1wk,t2wks,t3wks,t4wks\"\n",
    "n_input = 1\n",
    "\n",
    "# Build graph\n",
    "buildGraph(n_steps,n_input = n_input)\n",
    "# Initializing the variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "trainModel(xdTrain,ydTrain,sess,training_iterations)\n",
    "\n",
    "xdDummy = xdTrain.reshape(-1,1)\n",
    "predictions = getTestPredictions(xdTrain,ydTrain)\n",
    "print(metrics.classification_report(ydTrain[:,1],predictions))  # Need to feed it yTest not yTest_OneHot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '190Sales of shampoo over a three year period' does not match format '%Y-%m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   2931\u001b[0m                 result = tools.to_datetime(\n\u001b[0;32m-> 2932\u001b[0;31m                     date_parser(*date_cols), errors='ignore')\n\u001b[0m\u001b[1;32m   2933\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9bb8ef7c21d2>\u001b[0m in \u001b[0;36mparser\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'190'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: strptime() argument 1 must be str, not numpy.ndarray",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   2940\u001b[0m                                             \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2941\u001b[0;31m                                             dayfirst=dayfirst),\n\u001b[0m\u001b[1;32m   2942\u001b[0m                         errors='ignore')\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.try_parse_dates (pandas/_libs/lib.c:61574)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.try_parse_dates (pandas/_libs/lib.c:61472)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9bb8ef7c21d2>\u001b[0m in \u001b[0;36mparser\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'190'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    509\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    342\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 343\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '190Sales of shampoo over a three year period' does not match format '%Y-%m'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9bb8ef7c21d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3_Data/shampoo-sales.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# transform data to be stationary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m             \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m             \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0;31m# maybe create a mi on the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_index\u001b[0;34m(self, data, alldata, columns, indexnamerow)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_complex_date_col\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_simple_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_complex_date_col\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_agg_index\u001b[0;34m(self, index, try_parse_dates)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtry_parse_dates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_parse_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_date_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m             \u001b[0mcol_na_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   2942\u001b[0m                         errors='ignore')\n\u001b[1;32m   2943\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgeneric_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdate_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/date_converters.py\u001b[0m in \u001b[0;36mgeneric_parser\u001b[0;34m(parse_func, *cols)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9bb8ef7c21d2>\u001b[0m in \u001b[0;36mparser\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# date-time parsing function for loading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'190'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# frame a sequence as a supervised learning problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    509\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 343\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: time data '190Sales of shampoo over a three year period' does not match format '%Y-%m'"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# load dataset\n",
    "series = read_csv('3_Data/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "\n",
    "# transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    "\n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    "\n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "# repeat experiment\n",
    "repeats = 30\n",
    "error_scores = list()\n",
    "for r in range(repeats):\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\ttrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "\tlstm_model.predict(train_reshaped, batch_size=1)\n",
    "\t# walk-forward validation on the test data\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions.append(yhat)\n",
    "\t# report performance\n",
    "\trmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "\tprint('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "\terror_scores.append(rmse)\n",
    "\n",
    "# summarize results\n",
    "results = DataFrame()\n",
    "results['rmse'] = error_scores\n",
    "print(results.describe())\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
