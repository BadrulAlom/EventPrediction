{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Time series prediction using RNNs, with TensorFlow and Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating a Recurrent Neural Network in TensorFlow\n",
    "<li> Creating a Custom Estimator in tf.contrib.learn \n",
    "<li> Training on Cloud ML Engine\n",
    "</ol>\n",
    "\n",
    "<p>\n",
    "\n",
    "<h3> Simulate some time-series data </h3>\n",
    "\n",
    "Essentially a set of sinusoids with random amplitudes and frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/seaborn/timeseries.py:183: UserWarning: The tsplot function is deprecated and will be removed or replaced (in a substantially altered version) in a future release.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "SEQ_LEN = 10\n",
    "def create_time_series():\n",
    "  freq = (np.random.random()*0.5) + 0.1  # 0.1 to 0.6\n",
    "  ampl = np.random.random() + 0.5  # 0.5 to 1.5\n",
    "  x = np.sin(np.arange(0,SEQ_LEN) * freq) * ampl\n",
    "  return x\n",
    "\n",
    "for i in range(0, 5):\n",
    "  sns.tsplot( create_time_series() );  # 5 series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(filename, N):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    for lineno in range(0, N):\n",
    "      seq = create_time_series()\n",
    "      line = \",\".join(map(str, seq))\n",
    "      ofp.write(line + '\\n')\n",
    "\n",
    "to_csv('train.csv', 1000)  # 1000 sequences\n",
    "to_csv('valid.csv',  50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.csv <==\r\n",
      "0.0,0.35338388973,0.64822413822,0.835675786114,0.884684497484,0.787131202671,0.559177150843,0.238586542964,-0.121529706524,-0.461512634303\r\n",
      "0.0,0.529032858215,0.896850872135,0.991367007994,0.783778888227,0.337345884888,-0.211888227043,-0.696552573599,-0.968952950249,-0.946079419848\r\n",
      "0.0,0.475382483482,0.878944201643,1.14971506732,1.24678702969,1.15549446081,0.88962983809,0.489359977393,0.0151576321918,-0.461334727434\r\n",
      "0.0,0.60767499422,1.01468883705,1.08664095729,0.799771951583,0.248809207977,-0.384313148289,-0.89053095926,-1.10268538219,-0.950720628724\r\n",
      "0.0,0.50590629677,0.926106702271,1.18941481203,1.25122344016,1.10106154964,0.764368157028,0.298182693306,-0.218518080572,-0.69819957445\r\n",
      "\r\n",
      "==> valid.csv <==\r\n",
      "0.0,0.662382921387,1.12328967608,1.24252684553,0.983826212294,0.425876417362,-0.261611663182,-0.869525658468,-1.21295714545,-1.18744500762\r\n",
      "0.0,0.687311717573,1.21877062969,1.47386465962,1.3947494273,0.999364872637,0.377367246937,-0.330201077792,-0.962893966785,-1.37724382125\r\n",
      "0.0,0.356604642121,0.667861596925,0.894189823129,1.00680825503,0.991395751432,0.849912245849,0.600349511285,0.274443233471,-0.0863626627647\r\n",
      "0.0,0.493231165597,0.895578367703,1.13290414101,1.16147826647,0.976035606847,0.610746273602,0.132919359799,-0.369399595639,-0.803652091158\r\n",
      "0.0,0.410894114442,0.780328070337,1.07102513422,1.25365330564,1.30978498321,1.23375635194,1.0332388751,0.728465225989,0.350187764816\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 train.csv valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> RNN </h2>\n",
    "\n",
    "For more info, see:\n",
    "<ol>\n",
    "<li> http://colah.github.io/posts/2015-08-Understanding-LSTMs/ for the theory\n",
    "<li> https://www.tensorflow.org/tutorials/recurrent for explanations\n",
    "<li> https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb for sample code\n",
    "</ol>\n",
    "\n",
    "Here, we are trying to predict from 8 values of a timeseries, the next two values.\n",
    "\n",
    "<p>\n",
    "\n",
    "<h3> Imports </h3>\n",
    "\n",
    "Several tensorflow packages and shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Input Fn to read CSV </h3>\n",
    "\n",
    "Our CSV file structure is quite simple -- a bunch of floating point numbers (note the type of DEFAULTS). We ask for the data to be read BATCH_SIZE sequences at a time.  The Estimator API in tf.contrib.learn wants the features returned as a dict. We'll just call this timeseries column 'rawdata'.\n",
    "<p>\n",
    "Our CSV file sequences consist of 10 numbers. We'll assume that 8 of them are inputs and we need to predict the next two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULTS = [[0.0] for x in xrange(0, SEQ_LEN)]\n",
    "BATCH_SIZE = 20\n",
    "TIMESERIES_COL = 'rawdata'\n",
    "N_OUTPUTS = 2  # in each sequence, 1-8 are features, and 9-10 is label\n",
    "N_INPUTS = SEQ_LEN - N_OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data using the Estimator API in tf.learn requires an input_fn. This input_fn needs to return a dict of features and the corresponding labels.\n",
    "<p>\n",
    "So, we read the CSV file.  The Tensor format here will be batchsize x 1 -- entire line.  We then decode the CSV. At this point, all_data will contain a list of Tensors. Each tensor has a shape batchsize x 1.  There will be 10 of these tensors, since SEQ_LEN is 10.\n",
    "<p>\n",
    "We split these 10 into 8 and 2 (N_OUTPUTS is 2).  Put the 8 into a dict, call it features.  The other 2 are the ground truth, so labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode=tf.contrib.learn.ModeKeys.TRAIN):  \n",
    "  def _input_fn():\n",
    "    num_epochs = 100 if mode == tf.contrib.learn.ModeKeys.TRAIN else 1\n",
    "    \n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=True)\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=BATCH_SIZE)\n",
    "\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    print 'readcsv={}'.format(value_column)\n",
    "    \n",
    "    # all_data is a list of tensors\n",
    "    all_data = tf.decode_csv(value_column, record_defaults=DEFAULTS)  \n",
    "    inputs = all_data[:len(all_data)-N_OUTPUTS]  # first few values\n",
    "    label = all_data[len(all_data)-N_OUTPUTS : ] # last few values\n",
    "    \n",
    "    # from list of tensors to tensor with one more dimension\n",
    "    inputs = tf.concat(inputs, axis=1)\n",
    "    label = tf.concat(label, axis=1)\n",
    "    print 'inputs={}'.format(inputs)\n",
    "    \n",
    "    return {TIMESERIES_COL: inputs}, label   # dict of features, label\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define RNN </h3>\n",
    "\n",
    "A recursive neural network consists of possibly stacked LSTM cells.\n",
    "<p>\n",
    "The RNN has one output per input, so it will have 8 output cells.  We use only the last output cell, but rather use it directly, we do a matrix multiplication of that cell by a set of weights to get the actual predictions. This allows for a degree of scaling between inputs and predictions if necessary (we don't really need it in this problem).\n",
    "<p>\n",
    "Finally, to supply a model function to the Estimator API, you need to return a ModelFnOps. The rest of the function creates the necessary objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 3  # number of hidden layers in each of the LSTM cells\n",
    "\n",
    "# create the inference model\n",
    "def simple_rnn(features, targets, mode):\n",
    "  # 0. Reformat input shape to become a sequence\n",
    "  x = tf.split(features[TIMESERIES_COL], N_INPUTS, 1)\n",
    "  #print 'x={}'.format(x)\n",
    "    \n",
    "  # 1. configure the RNN\n",
    "  lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)\n",
    "  outputs, _ = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "  # slice to keep only the last cell of the RNN\n",
    "  outputs = outputs[-1]\n",
    "  #print 'last outputs={}'.format(outputs)\n",
    "  \n",
    "  # output is result of linear activation of last layer of RNN\n",
    "  weight = tf.Variable(tf.random_normal([LSTM_SIZE, N_OUTPUTS]))\n",
    "  bias = tf.Variable(tf.random_normal([N_OUTPUTS]))\n",
    "  predictions = tf.matmul(outputs, weight) + bias\n",
    "    \n",
    "  # 2. loss function, training/eval ops\n",
    "  if mode == tf.contrib.learn.ModeKeys.TRAIN or mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "     loss = tf.losses.mean_squared_error(targets, predictions)\n",
    "     train_op = tf.contrib.layers.optimize_loss(\n",
    "         loss=loss,\n",
    "         global_step=tf.contrib.framework.get_global_step(),\n",
    "         learning_rate=0.01,\n",
    "         optimizer=\"SGD\")\n",
    "     eval_metric_ops = {\n",
    "      \"rmse\": tf.metrics.root_mean_squared_error(targets, predictions)\n",
    "     }\n",
    "  else:\n",
    "     loss = None\n",
    "     train_op = None\n",
    "     eval_metric_ops = None\n",
    "  \n",
    "  # 3. Create predictions\n",
    "  predictions_dict = {\"predicted\": predictions}\n",
    "  \n",
    "  # 4. return ModelFnOps\n",
    "  return tflearn.ModelFnOps(\n",
    "      mode=mode,\n",
    "      predictions=predictions_dict,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Experiment </h3>\n",
    "\n",
    "Distributed training is launched off using an Experiment.  The key line here is that we use tflearn.Estimator rather than, say tflearn.DNNRegressor.  This allows us to provide a model_fn, which will be our RNN defined above.  Note also that we specify a serving_input_fn -- this is how we parse the input data provided to us at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train():\n",
    "  return read_dataset('train.csv', mode=tf.contrib.learn.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid():\n",
    "  return read_dataset('valid.csv', mode=tf.contrib.learn.ModeKeys.EVAL)\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "  \n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "    \n",
    "    print 'serving: features={}'.format(features[TIMESERIES_COL])\n",
    "    \n",
    "    return tflearn.utils.input_fn_utils.InputFnOps(\n",
    "      features,\n",
    "      None,\n",
    "      feature_placeholders\n",
    "    )\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "def experiment_fn(output_dir):\n",
    "    # run experiment\n",
    "    return tflearn.Experiment(\n",
    "        tflearn.Estimator(model_fn=simple_rnn, model_dir=output_dir),\n",
    "        train_input_fn=get_train(),\n",
    "        eval_input_fn=get_valid(),\n",
    "        eval_metrics={\n",
    "            'rmse': tflearn.MetricSpec(\n",
    "                metric_fn=metrics.streaming_root_mean_squared_error\n",
    "            )\n",
    "        },\n",
    "        export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "            serving_input_fn,\n",
    "            default_output_alternative_key=None,\n",
    "            exports_to_keep=1\n",
    "        )]\n",
    "    )\n",
    "\n",
    "shutil.rmtree('outputdir', ignore_errors=True) # start fresh each time\n",
    "learn_runner.run(experiment_fn, 'outputdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Standalone Python module </h3>\n",
    "\n",
    "To train this on Cloud ML Engine, we take the code in this notebook, make an standalone Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "# run module as-is\n",
    "REPO=$(pwd)\n",
    "echo $REPO\n",
    "rm -rf outputdir\n",
    "export PYTHONPATH=${PYTHONPATH}:${REPO}/simplernn\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"${REPO}/train.csv*\" \\\n",
    "   --eval_data_paths=\"${REPO}/valid.csv*\"  \\\n",
    "   --output_dir=${REPO}/outputdir \\\n",
    "   --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out online prediction.  This is how the REST API will work after you train on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile test.json\n",
    "{\"rawdata\": [0.0,0.0527,0.10498,0.1561,0.2056,0.253,0.2978,0.3395]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "- predicted:\n",
      "  - 0.456365\n",
      "  - 0.48135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 2017-06-27 17:52:12.098509: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-06-27 17:52:12.098577: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-06-27 17:52:12.098596: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "WARNING:root:MetaGraph has multiple signatures 2. Support for multiple signatures is limited. By default we select named signatures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_DIR=$(ls ./outputdir/export/Servo/)\n",
    "gcloud ml-engine local predict --model-dir=./outputdir/export/Servo/$MODEL_DIR --json-instances=test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cloud ML Engine </h3>\n",
    "\n",
    "Now to train on Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "# run module on Cloud ML Engine\n",
    "REPO=$(pwd)\n",
    "BUCKET=cloud-training-demos-ml # CHANGE AS NEEDED\n",
    "OUTDIR=gs://${BUCKET}/simplernn/model_trained\n",
    "JOBNAME=simplernn_$(date -u +%y%m%d_%H%M%S)\n",
    "REGION=us-central1\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${REPO}/simplernn/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC \\\n",
    "   --runtime-version=1.2 \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/train.csv*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/valid.csv*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --num_epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Variant: long sequence </h2>\n",
    "\n",
    "To create short sequences from a very long sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input= [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "output= [[ 1.  2.  3.  4.  5.]\n",
      " [ 2.  3.  4.  5.  6.]\n",
      " [ 3.  4.  5.  6.  7.]\n",
      " [ 4.  5.  6.  7.  8.]\n",
      " [ 5.  6.  7.  8.  9.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def breakup(sess, x, lookback_len):\n",
    "  N = sess.run(tf.size(x))\n",
    "  windows = [tf.slice(x, [b], [lookback_len]) for b in xrange(0, N-lookback_len)]\n",
    "  windows = tf.stack(windows)\n",
    "  return windows\n",
    "\n",
    "x = tf.constant(np.arange(1,11, dtype=np.float32))\n",
    "with tf.Session() as sess:\n",
    "    print 'input=', x.eval()\n",
    "    seqx = breakup(sess, x, 5)\n",
    "    print 'output=', seqx.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
